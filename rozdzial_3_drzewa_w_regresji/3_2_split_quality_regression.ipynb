{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3.2 Pomiar jakości podziału dla regresji\n",
    "W przypadku klasyfikacji korzystaliśmy z proporcji klas, aby obliczyć pewne metryki jakości. Dla regresji nie jest to oczywiście możliwe, bo wartości numeryczne mogą być zupełnie unikalne. Nie da się ich więc wykorzystać jako etykiet, bo doszlibyśmy do sytuacji, w której każda obserwacja należałaby do swojej własnej klasy, przez co zdolności do generalizowania naszego modelu byłyby mizerne.\n",
    "\n",
    "Okazuje się jednak, że możemy wykorzystać ten sam schemat nauczania, aby rozwiązać także problemy regresji, co czyni algorytm drzew decyzyjnych bardzo użytecznym w obu problemach. Przypomnijmy sobie ogólny schemat nauczania, a później trochę formalnie zdefiniujmy funkcję *impurity measure*."
   ],
   "id": "5951ac08c35c283c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Proces uczenia drzewa\n",
    "\n",
    "Formalnie, proces podziału drzewa $ Q $ możemy określić w następujący sposób:\n",
    "\n",
    "$ \\theta = (j, t_{m}) $ jest podziałem drzewa w węźle $ m $ ze względu na cechę $ j $ i wartość $ t_{m} $ określa próg dla tej cechy. Jeśli wektory wejściowe oznaczymy jako $ x \\in R^{n} $, a ich klasy jako $ y $, to podział $ \\theta $ tworzy warunek $ x_{j} <= t_{m} $. Taki podział tworzy nam dwa drzewa:\n",
    "\n",
    "$$ Q_{left}(\\theta) = (x, y) | x_{j} <= t_{m} $$\n",
    "\n",
    "$$ Q_{right}(\\theta) = (x, y) | x_{j} > t_{m} = Q \\setminus Q_{left}(\\theta) $$\n",
    "\n",
    "Jak wybrać najlepszy podział? Musimy odnaleźć taki, który minimalizuje nam tzw. *impurity* czyli nieczystość poddrzew. Obliczamy go w następujący sposób:\n",
    "\n",
    "$$ G(Q, \\theta) = \\frac{n_{left}}{N_{m}} H(Q_{left}(\\theta)) + \\frac{n_{right}}{N_{m}} H(Q_{right}(\\theta)) $$\n",
    "\n",
    "$ H $ to tzw. *impurity function*, której wybór jest jednym z parametrów algorytmu uczenia drzewa. Na razie zauważmy tylko, że wartość tej funkcji osiąga zero, wtedy gdy wszystkie elementy w danym poddrzewie należą do tej samej klasy. Nieczystość takiego drzewa jest rzeczywiście zerowa. Wyraz $ G $ jest więc sumą ważoną nieczystości poszczególnych poddrzew, a ich wagi zależą od liczby zawartych w nich elementów. \n",
    "\n",
    "Korzystając z wartości $ G $ jesteśmy w stanie obliczyć nieczystość jaka zostanie utworzona poprzez każdy z podziałów jakie rozpatrujemy, a dzięki temu możemy wybrać taki podział, który będzie miał jak najmniejszą nieczystość, czyli najlepiej pogrupuje nasze obserwacje.\n",
    "\n",
    "$$ \\theta^{*} = argmin_{\\theta}G(Q, \\theta) $$\n",
    "\n",
    "Po każdorazowym podziale, otrzymujemy dwa poddrzewa, wraz z przypisanymi do nich obserwacjami. Możemy je potraktować jak zupełnie osobne podproblemy i nie patrzeć na poprzednie podziały, a co za tym idzie rozwiązywać w sposób rekurencyjny, aż do osiągnięcia maksymalnej głębokości drzewa bądź też minimalnej dopuszczalnej liczby przykładów zawartych w danym drzewie, które również są parametrami algorytmu uczenia.\n"
   ],
   "id": "ca759c0f924d424"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Impurity function dla regresji\n",
    "\n",
    "Jak wspomnieliśmy, szukamy funkcji $ H $, ale dla regresji musimy skorzystać z innych miar niż w przypadku klasyfikacji. Intuicyjne wydaje się być skorzystanie z miar błędu regresji, który już powinieneś na tym etapie znać. Wartości **Mean Squared Error** oraz **Mean Absolute Error** są często optymalizowane w przewidywaniu wartości numerycznych i również przyjmują wartości zerowe, jeśli nasze predykcje są perfekcyjne, a rosną wraz z popełnianymi błędami. Są więc równie często używane przy uczeniu drzew decyzyjnych, ale przyjrzymy się im dokładniej w kolejnych podrozdziałach."
   ],
   "id": "30175fab9a3e345b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4736e49ef185a6b7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
